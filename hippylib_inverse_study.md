# hippylib åé—®é¢˜åœºå‚æ•°ä¼°è®¡æ·±åº¦å­¦ä¹ ç¬”è®°

## ç›®å½•
1. [åº“æ¦‚è¿°](#1-åº“æ¦‚è¿°)
2. [æ ¸å¿ƒæ•°å­¦ç†è®º](#2-æ ¸å¿ƒæ•°å­¦ç†è®º)
3. [æ ¸å¿ƒæ¨¡å—è¯¦è§£](#3-æ ¸å¿ƒæ¨¡å—è¯¦è§£)
4. [åé—®é¢˜æ±‚è§£æµç¨‹](#4-åé—®é¢˜æ±‚è§£æµç¨‹)
5. [å…³é”®ç®—æ³•å®ç°](#5-å…³é”®ç®—æ³•å®ç°)
6. [å®è·µåº”ç”¨æŒ‡å—](#6-å®è·µåº”ç”¨æŒ‡å—)

---

## 1. åº“æ¦‚è¿°

### 1.1 hippylib ç®€ä»‹
**hippylib** (Inverse Problem PYthon library) æ˜¯ä¸“é—¨ç”¨äº PDE çº¦æŸçš„ç¡®å®šæ€§å’Œè´å¶æ–¯åé—®é¢˜çš„åº“ã€‚

**æ ¸å¿ƒç‰¹æ€§**ï¼š
- ğŸ”§ åŸºäº **FEniCS** è¿›è¡Œ PDE ç¦»æ•£åŒ–ï¼ˆæœ‰é™å…ƒæ–¹æ³•ï¼‰
- âš¡ åŸºäº **PETSc** è¿›è¡Œå¹¶è¡Œçº¿æ€§ä»£æ•°è¿ç®—
- ğŸ“ˆ ç»´åº¦ç‹¬ç«‹çš„ç®—æ³•ï¼ˆæˆæœ¬ä¸éšå‚æ•°ç»´åº¦å¢åŠ ï¼‰
- ğŸ¯ è‡ªåŠ¨å¾®åˆ†è®¡ç®—æ¢¯åº¦å’Œ Hessian
- ğŸ” ä½ç§© Hessian è¿‘ä¼¼ç”¨äºåéªŒåæ–¹å·®

**é€‚ç”¨é—®é¢˜**ï¼š
- å‚æ•°è¯†åˆ«ï¼ˆcoefficient field inversionï¼‰
- ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆuncertainty quantificationï¼‰
- è´å¶æ–¯åæ¼”ï¼ˆBayesian inferenceï¼‰
- æ•°æ®åŒåŒ–ï¼ˆdata assimilationï¼‰

---

## 2. æ ¸å¿ƒæ•°å­¦ç†è®º

### 2.1 åé—®é¢˜çš„æ•°å­¦æè¿°

#### æ­£é—®é¢˜ï¼ˆForward Problemï¼‰
ç»™å®šå‚æ•° $m$ï¼Œæ±‚è§£çŠ¶æ€ $u$ï¼š
$$
\begin{cases}
-\nabla \cdot (a(m) \nabla u) = f & \text{in } \Omega \\
u = u_0 & \text{on } \partial\Omega
\end{cases}
$$

å¸¸è§å½¢å¼ï¼š$a(m) = \exp(m)$ æˆ– $a(m) = m$

#### åé—®é¢˜ï¼ˆInverse Problemï¼‰
ç»™å®šè§‚æµ‹æ•°æ® $u_d$ï¼Œæ¢å¤å‚æ•° $m$ï¼š
$$
\min_{m} J(m) = \underbrace{\frac{1}{2}\|u(m) - u_d\|^2_{W}}_{\text{misfit}} + \underbrace{\frac{\gamma}{2}\|\nabla m\|^2}_{\text{regularization}}
$$

å…¶ä¸­ï¼š
- $u(m)$ï¼šæ­£é—®é¢˜çš„è§£
- $u_d$ï¼šè§‚æµ‹æ•°æ®ï¼ˆå«å™ªå£°ï¼‰
- $\gamma$ï¼šæ­£åˆ™åŒ–å‚æ•°
- $W$ï¼šè¯¯å·®æƒé‡çŸ©é˜µ

### 2.2 æ‹‰æ ¼æœ—æ—¥æ–¹æ³•ä¸ä¼´éšæ–¹æ³•

#### æ‹‰æ ¼æœ—æ—¥å‡½æ•°
$$
\mathcal{L}(u,m,p) = \frac{1}{2}(u-u_d, u-u_d) + \frac{\gamma}{2}(\nabla m, \nabla m) + (\exp(m)\nabla u, \nabla p) - (f, p)
$$

å…¶ä¸­ $p$ æ˜¯æ‹‰æ ¼æœ—æ—¥ä¹˜å­ï¼ˆä¼´éšå˜é‡ï¼‰ã€‚

#### æœ€ä¼˜æ€§æ¡ä»¶ï¼ˆKKTæ¡ä»¶ï¼‰
1. **çŠ¶æ€æ–¹ç¨‹ï¼ˆæ­£é—®é¢˜ï¼‰**ï¼š
   $$\mathcal{L}_p = 0 \Rightarrow (\exp(m)\nabla u, \nabla \tilde{p}) = (f, \tilde{p}), \quad \forall \tilde{p}$$

2. **ä¼´éšæ–¹ç¨‹**ï¼š
   $$\mathcal{L}_u = 0 \Rightarrow (\exp(m)\nabla p, \nabla \tilde{u}) = -(u-u_d, \tilde{u}), \quad \forall \tilde{u}$$

3. **æ¢¯åº¦æ–¹ç¨‹**ï¼š
   $$\mathcal{L}_m = 0 \Rightarrow \gamma(\nabla m, \nabla \tilde{m}) + (\tilde{m}\exp(m)\nabla u, \nabla p) = 0, \quad \forall \tilde{m}$$

#### æ¢¯åº¦çš„æ˜¾å¼è¡¨è¾¾
$$
\nabla_m J(m) = \gamma \nabla^2 m + \exp(m)(\nabla u \cdot \nabla p)
$$

**å…³é”®ä¼˜åŠ¿**ï¼šåªéœ€æ±‚è§£ä¸€æ¬¡æ­£é—®é¢˜å’Œä¸€æ¬¡ä¼´éšé—®é¢˜ï¼Œå³å¯å¾—åˆ°å®Œæ•´æ¢¯åº¦ï¼ˆä¸å‚æ•°ç»´åº¦æ— å…³ï¼‰ã€‚

### 2.3 Hessian çŸ©é˜µçš„ç»“æ„

#### å®Œæ•´ Hessianï¼ˆNewton æ–¹æ³•ï¼‰
$$
\mathcal{H}(m)(\hat{m}) = \gamma \nabla^2 \hat{m} + C^T W_{uu} C + C^T W_{um} + W_{mu} C + W_{mm}
$$

å…¶ä¸­ï¼š
- $C = \frac{\partial u}{\partial m}$ï¼šå‚æ•°-çŠ¶æ€çš„çµæ•åº¦çŸ©é˜µ
- $W_{uu}$ï¼šçŠ¶æ€-çŠ¶æ€äºŒé˜¶å¯¼æ•°
- $W_{um}, W_{mu}$ï¼šæ··åˆäºŒé˜¶å¯¼æ•°
- $W_{mm}$ï¼šå‚æ•°-å‚æ•°äºŒé˜¶å¯¼æ•°

#### Gauss-Newton è¿‘ä¼¼
å¿½ç•¥äºŒé˜¶é¡¹ $W_{um}, W_{mu}, W_{mm}$ï¼š
$$
\mathcal{H}_{GN}(m)(\hat{m}) = \gamma \nabla^2 \hat{m} + C^T W_{uu} C
$$

**ä½•æ—¶ä½¿ç”¨ Gauss-Newton**ï¼š
- å‰å‡ æ¬¡è¿­ä»£ï¼ˆè¿œç¦»æœ€ä¼˜ç‚¹ï¼‰
- å™ªå£°æ°´å¹³è¾ƒé«˜
- è®¡ç®—æˆæœ¬è¦æ±‚è¾ƒä½

### 2.4 Hessian-å‘é‡ä¹˜ç§¯ï¼ˆæ— çŸ©é˜µæ–¹æ³•ï¼‰

è®¡ç®— $H\hat{m}$ éœ€è¦æ±‚è§£ä¸¤ä¸ªçº¿æ€§ç³»ç»Ÿï¼š

1. **å¢é‡æ­£é—®é¢˜**ï¼š
   $$(\exp(m)\nabla \hat{u}, \nabla \tilde{p}) = -(\hat{m}\exp(m)\nabla u, \nabla \tilde{p})$$

2. **å¢é‡ä¼´éšé—®é¢˜**ï¼š
   $$(\exp(m)\nabla \hat{p}, \nabla \tilde{u}) = -(W_{uu}\hat{u}, \tilde{u}) - (W_{um}\hat{m}, \tilde{u})$$

3. **ç»„è£…ç»“æœ**ï¼š
   $$H\hat{m} = \gamma R\hat{m} + C^T\hat{p} + W_{mu}\hat{u} + W_{mm}\hat{m}$$

---

## 3. æ ¸å¿ƒæ¨¡å—è¯¦è§£

### 3.1 æ¨¡å—æ¶æ„

```
hippylib/
â”œâ”€â”€ modeling/               # å»ºæ¨¡æ ¸å¿ƒ
â”‚   â”œâ”€â”€ model.py           # Model ç±»ï¼šæ•´åˆæ‰€æœ‰ç»„ä»¶
â”‚   â”œâ”€â”€ prior.py           # å…ˆéªŒåˆ†å¸ƒï¼ˆLaplacianPrior, BiLaplacianPriorï¼‰
â”‚   â”œâ”€â”€ misfit.py          # ç›®æ ‡å‡½æ•°ï¼ˆDiscreteStateObservationï¼‰
â”‚   â”œâ”€â”€ PDEProblem.py      # PDE é—®é¢˜æŠ½è±¡åŸºç±»
â”‚   â”œâ”€â”€ PDEVariationalProblem.py  # å˜åˆ†å½¢å¼çš„ PDE
â”‚   â”œâ”€â”€ reducedHessian.py  # Reduced Hessian ç®—å­
â”‚   â””â”€â”€ posterior.py       # åéªŒåˆ†å¸ƒä¸ä¸ç¡®å®šæ€§é‡åŒ–
â”‚
â”œâ”€â”€ algorithms/            # ä¼˜åŒ–ç®—æ³•
â”‚   â”œâ”€â”€ NewtonCG.py        # Inexact Newton-CG æ–¹æ³•
â”‚   â”œâ”€â”€ cgsolverSteihaug.py  # Steihaug-CG æ±‚è§£å™¨
â”‚   â”œâ”€â”€ randomizedEigensolver.py  # ä½ç§©ç‰¹å¾å€¼æ±‚è§£
â”‚   â””â”€â”€ linalg.py          # çº¿æ€§ä»£æ•°å·¥å…·
â”‚
â”œâ”€â”€ mcmc/                  # MCMC é‡‡æ ·
â”œâ”€â”€ utils/                 # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ random.py          # å¹¶è¡Œéšæœºæ•°ç”Ÿæˆ
â”‚   â””â”€â”€ finite_diff.py     # æœ‰é™å·®åˆ†å·¥å…·
â””â”€â”€ forward_uq/            # å‰å‘ä¸ç¡®å®šæ€§é‡åŒ–
```

### 3.2 Model ç±»è¯¦è§£

`Model` æ˜¯ hippylib çš„æ ¸å¿ƒç±»ï¼Œæ•´åˆäº†æ‰€æœ‰ç»„ä»¶ï¼š

```python
class Model:
    def __init__(self, problem, prior, misfit):
        self.problem = problem  # PDEProblem: å®šä¹‰ PDE
        self.prior = prior      # Prior: å…ˆéªŒåˆ†å¸ƒ
        self.misfit = misfit    # Misfit: ç›®æ ‡å‡½æ•°
        self.gauss_newton_approx = False
```

**ä¸»è¦æ–¹æ³•**ï¼š

1. **generate_vector(component)**: ç”Ÿæˆé€‚å½“å½¢çŠ¶çš„å‘é‡
   ```python
   x = model.generate_vector()  # è¿”å› [u, m, p]
   m = model.generate_vector(PARAMETER)
   ```

2. **solveFwd(out, x)**: æ±‚è§£æ­£é—®é¢˜
   - è¾“å…¥ï¼š`x = [u_init, m, p]`ï¼ˆä»…ä½¿ç”¨ m å’Œ u_initï¼‰
   - è¾“å‡ºï¼š`out`ï¼ˆæ±‚è§£çš„çŠ¶æ€ uï¼‰

3. **solveAdj(out, x)**: æ±‚è§£ä¼´éšé—®é¢˜
   - è¾“å…¥ï¼š`x = [u, m, p]`ï¼ˆä½¿ç”¨ u å’Œ mï¼‰
   - è¾“å‡ºï¼š`out`ï¼ˆæ±‚è§£çš„ä¼´éš pï¼‰

4. **evalGradientParameter(x, mg)**: è®¡ç®—æ¢¯åº¦
   - è¿”å›ï¼šæ¢¯åº¦èŒƒæ•°

5. **setPointForHessianEvaluations(x, gauss_newton_approx)**: è®¾ç½® Hessian è¯„ä¼°ç‚¹

6. **solveFwdIncremental(sol, rhs)**: æ±‚è§£å¢é‡æ­£é—®é¢˜

7. **solveAdjIncremental(sol, rhs)**: æ±‚è§£å¢é‡ä¼´éšé—®é¢˜

### 3.3 Prior ç±»è¯¦è§£

#### LaplacianPrior

åæ–¹å·®ç®—å­ï¼š$C = (\delta I - \gamma \Delta)^{-1}$

```python
prior = LaplacianPrior(
    Vh,                    # æœ‰é™å…ƒç©ºé—´
    gamma=0.1,            # æ§åˆ¶ç›¸å…³é•¿åº¦
    delta=0.5,            # æ§åˆ¶æ–¹å·®
    mean=None,            # å…ˆéªŒå‡å€¼
    rel_tol=1e-12,        # æ±‚è§£å™¨å®¹å·®
    solver_type="krylov"  # æˆ– "lu"
)
```

**å…³é”®å‚æ•°**ï¼š
- $\gamma$ï¼šè¶Šå¤§ï¼Œç›¸å…³é•¿åº¦è¶Šé•¿ï¼ˆåœºè¶Šå…‰æ»‘ï¼‰
- $\delta$ï¼šè¶Šå¤§ï¼Œæ–¹å·®è¶Šå°ï¼ˆåœºè¶Šæ¥è¿‘å‡å€¼ï¼‰
- æ¯”å€¼ $\gamma/\delta$ æ§åˆ¶ç›¸å…³é•¿åº¦å°ºåº¦

**ä¸»è¦æ–¹æ³•**ï¼š
```python
# é‡‡æ ·
noise = dl.Vector()
prior.init_vector(noise, "noise")
parRandom.normal(1., noise)
m_sample = dl.Vector()
prior.init_vector(m_sample, 0)
prior.sample(noise, m_sample, add_mean=True)

# è®¡ç®—å…ˆéªŒæˆæœ¬
cost = prior.cost(m)  # 0.5 * (m - m_mean)^T R (m - m_mean)

# è®¡ç®—æ¢¯åº¦
grad = dl.Vector()
prior.init_vector(grad, 0)
prior.grad(m, grad)  # grad = R * (m - m_mean)
```

#### BiLaplacianPrior

åæ–¹å·®ç®—å­ï¼š$C = (\delta I - \gamma \Delta)^{-2}$ï¼ˆæ›´å…‰æ»‘ï¼‰

```python
prior = BiLaplacianPrior(
    Vh, gamma, delta,
    anis_diff=None,     # å„å‘å¼‚æ€§æ‰©æ•£å¼ é‡
    robin_bc=True       # Robin è¾¹ç•Œæ¡ä»¶
)
```

### 3.4 Misfit ç±»è¯¦è§£

#### DiscreteStateObservation

ç”¨äºç‚¹è§‚æµ‹æˆ–ç¦»æ•£è§‚æµ‹ï¼š

```python
# æ„å»ºè§‚æµ‹ç®—å­ B
B = assemblePointwiseObservation(Vh, obs_points)

# åˆ›å»º misfit
misfit = DiscreteStateObservation(
    B,                    # è§‚æµ‹ç®—å­
    data=d_obs,          # è§‚æµ‹æ•°æ®
    noise_variance=0.01  # å™ªå£°æ–¹å·®
)
```

**ä¸»è¦æ–¹æ³•**ï¼š
```python
# è®¡ç®— misfit cost
cost = misfit.cost(x)  # 0.5/noise_var * ||B*u - d||^2

# è®¡ç®—æ¢¯åº¦
grad_u = model.generate_vector(STATE)
misfit.grad(STATE, x, grad_u)  # B^T * (B*u - d) / noise_var

grad_m = model.generate_vector(PARAMETER)
misfit.grad(PARAMETER, x, grad_m)  # é€šå¸¸ä¸º 0

# è®¾ç½®çº¿æ€§åŒ–ç‚¹
misfit.setLinearizationPoint(x, gauss_newton_approx=False)

# åº”ç”¨ Hessian å—
out = model.generate_vector(STATE)
misfit.apply_ij(STATE, STATE, dir, out)  # W_uu * dir
```

### 3.5 ReducedHessian ç±»è¯¦è§£

æ— çŸ©é˜µå®ç°çš„ Hessian ç®—å­ï¼š

```python
class ReducedHessian:
    def __init__(self, model, misfit_only=False):
        self.model = model
        self.misfit_only = misfit_only
        self.ncalls = 0  # è®°å½•è°ƒç”¨æ¬¡æ•°
    
    def mult(self, x, y):
        """åº”ç”¨ Hessian: y = H * x"""
        if self.gauss_newton_approx:
            self.GNHessian(x, y)
        else:
            self.TrueHessian(x, y)
        self.ncalls += 1
    
    def GNHessian(self, x, y):
        """Gauss-Newton è¿‘ä¼¼"""
        # 1. å¢é‡æ­£é—®é¢˜ï¼šC * x
        self.model.applyC(x, self.rhs_fwd)
        self.model.solveFwdIncremental(self.uhat, self.rhs_fwd)
        
        # 2. å¢é‡ä¼´éšé—®é¢˜ï¼šW_uu * uhat
        self.model.applyWuu(self.uhat, self.rhs_adj)
        self.model.solveAdjIncremental(self.phat, self.rhs_adj)
        
        # 3. C^T * phat
        self.model.applyCt(self.phat, y)
        
        # 4. åŠ ä¸Šæ­£åˆ™åŒ–é¡¹
        if not self.misfit_only:
            self.model.applyR(x, self.yhelp)
            y.axpy(1., self.yhelp)
    
    def TrueHessian(self, x, y):
        """å®Œæ•´ Newton Hessian"""
        # ç±»ä¼¼ GNHessianï¼Œä½†åŒ…å«äºŒé˜¶é¡¹
        # W_um, W_mu, W_mm
```

---

## 4. åé—®é¢˜æ±‚è§£æµç¨‹

### 4.1 å®Œæ•´å·¥ä½œæµç¨‹

```python
# ===== æ­¥éª¤ 1: è®¾ç½®ç½‘æ ¼å’Œå‡½æ•°ç©ºé—´ =====
mesh = dl.UnitSquareMesh(64, 64)
Vh_state = dl.FunctionSpace(mesh, 'Lagrange', 2)  # çŠ¶æ€ï¼šäºŒé˜¶
Vh_param = dl.FunctionSpace(mesh, 'Lagrange', 1)  # å‚æ•°ï¼šä¸€é˜¶
Vh = [Vh_state, Vh_param, Vh_state]

# ===== æ­¥éª¤ 2: å®šä¹‰æ­£é—®é¢˜ =====
def u_boundary(x, on_boundary):
    return on_boundary

bc = dl.DirichletBC(Vh[STATE], dl.Constant(0.0), u_boundary)
bc0 = dl.DirichletBC(Vh[STATE], dl.Constant(0.0), u_boundary)

f = dl.Constant(1.0)

def pde_varf(u, m, p):
    return ufl.exp(m) * ufl.inner(ufl.grad(u), ufl.grad(p)) * ufl.dx - f * p * ufl.dx

pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)

# ===== æ­¥éª¤ 3: å®šä¹‰å…ˆéªŒ =====
gamma = 0.1
delta = 0.5
prior = BiLaplacianPrior(Vh[PARAMETER], gamma, delta)

# ç”ŸæˆçœŸå®å‚æ•°
noise = dl.Vector()
prior.init_vector(noise, "noise")
parRandom.normal(1., noise)
mtrue = dl.Vector()
prior.init_vector(mtrue, 0)
prior.sample(noise, mtrue)

# ===== æ­¥éª¤ 4: ç”Ÿæˆåˆæˆè§‚æµ‹æ•°æ® =====
utrue = pde.generate_state()
x = [utrue, mtrue, None]
pde.solveFwd(utrue, x)

# æ„å»ºè§‚æµ‹ç®—å­
ntargets = 100
np.random.seed(1)
targets = np.random.uniform(0.1, 0.9, [ntargets, 2])
B = assemblePointwiseObservation(Vh[STATE], targets)

# æ·»åŠ å™ªå£°
rel_noise = 0.01
utrue_obs = dl.Vector()
B.init_vector(utrue_obs, 0)
B.mult(utrue, utrue_obs)
noise_level = rel_noise * utrue_obs.norm("linf")
noise_vec = dl.Vector()
B.init_vector(noise_vec, 0)
parRandom.normal(noise_level, noise_vec)
data = utrue_obs + noise_vec

# ===== æ­¥éª¤ 5: å®šä¹‰ misfit =====
noise_variance = noise_level**2
misfit = DiscreteStateObservation(B, data, noise_variance)

# ===== æ­¥éª¤ 6: æ„å»º Model =====
model = Model(pde, prior, misfit)

# ===== æ­¥éª¤ 7: è®¾ç½®åˆå§‹çŒœæµ‹ =====
m0 = dl.interpolate(dl.Constant(0.0), Vh[PARAMETER])
x = model.generate_vector()
x[STATE].zero()
x[PARAMETER].axpy(1., m0.vector())

# ===== æ­¥éª¤ 8: ä¼˜åŒ–æ±‚è§£ =====
parameters = ReducedSpaceNewtonCG_ParameterList()
parameters["rel_tolerance"] = 1e-6
parameters["abs_tolerance"] = 1e-9
parameters["max_iter"] = 20
parameters["GN_iter"] = 5  # å‰5æ¬¡ç”¨ Gauss-Newton
parameters["globalization"] = "LS"  # çº¿æœç´¢
parameters["cg_coarse_tolerance"] = 0.5

solver = ReducedSpaceNewtonCG(model, parameters)

# æ±‚è§£
x = solver.solve(x)

print("Converged:", solver.converged)
print("Reason:", ReducedSpaceNewtonCG.termination_reasons[solver.reason])
print("Iterations:", solver.it)
print("Total CG iterations:", solver.total_cg_iter)
print("Final gradient norm:", solver.final_grad_norm)

# ===== æ­¥éª¤ 9: å¯è§†åŒ–ç»“æœ =====
m_MAP = x[PARAMETER]
u_MAP = x[STATE]

# ç»˜å›¾
plt.figure(figsize=(15, 5))
plt.subplot(131)
dl.plot(dl.Function(Vh[PARAMETER], mtrue))
plt.title("True Parameter")
plt.subplot(132)
dl.plot(dl.Function(Vh[PARAMETER], m_MAP))
plt.title("MAP Estimate")
plt.subplot(133)
dl.plot(dl.Function(Vh[PARAMETER], mtrue - m_MAP))
plt.title("Error")
plt.show()
```

### 4.2 hippytest.py çš„å®ç°ç»†èŠ‚

ä½ çš„ä»£ç æ‰‹åŠ¨å®ç°äº†æ ¸å¿ƒç®—æ³•ï¼Œè®©æˆ‘ä»¬å¯¹ç…§ç†è§£ï¼š

```python
# ä½ çš„å®ç°                          # hippylib ç­‰ä»·
# ============================================================

# 1. æ¢¯åº¦è®¡ç®—
CT_p = dl.Vector()
C.init_vector(CT_p, 1)
C.transpmult(p.vector(), CT_p)      # C^T * p
MG = CT_p + R * m.vector()          # C^T*p + R*mï¼ˆæ­£åˆ™åŒ–é¡¹ï¼‰
dl.solve(M, g, MG)                  # g = M^{-1} * MGï¼ˆé¢„æ¡ä»¶ï¼‰

# hippylib ä¸­ï¼š
gradnorm = model.evalGradientParameter(x, mg)

# 2. Hessian åº”ç”¨ï¼ˆGauss-Newtonï¼‰
# å¢é‡æ­£é—®é¢˜
rhs = -(self.C * v)
bc_adj.apply(rhs)
dl.solve(self.A, self.du, rhs)      # æ±‚è§£ A * du = -C * v

# å¢é‡ä¼´éšé—®é¢˜
rhs = -(self.W * self.du)
bc_adj.apply(rhs)
dl.solve(self.adj_A, self.dp, rhs)  # æ±‚è§£ A_adj * dp = -W * du

# ç»„è£… Hessian
self.R.mult(v, y)                   # æ­£åˆ™åŒ–é¡¹
self.C.transpmult(self.dp, self.CT_dp)
y.axpy(1, self.CT_dp)               # åŠ ä¸Š C^T * dp

# hippylib ä¸­ï¼š
HessApply = ReducedHessian(model)
HessApply.mult(mhat, result)

# 3. Newton-CG ä¼˜åŒ–
solver = CGSolverSteihaug()
solver.set_operator(Hess_Apply)
solver.set_preconditioner(Psolver)
solver.solve(m_delta, -MG)          # æ±‚è§£ H * m_delta = -g

# hippylib ä¸­ï¼š
solver = ReducedSpaceNewtonCG(model, parameters)
x = solver.solve(x)

# 4. çº¿æœç´¢
while descent == 0 and no_backtrack < 10:
    m.vector().axpy(alpha, m_delta)
    # æ±‚è§£æ­£é—®é¢˜
    # æ£€æŸ¥ Armijo æ¡ä»¶
    if cost_new < cost_old + alpha * c * MG.inner(m_delta):
        descent = 1
    else:
        alpha *= 0.5

# hippylib è‡ªåŠ¨å¤„ç†
```

---

## 5. å…³é”®ç®—æ³•å®ç°

### 5.1 Inexact Newton-CG ç®—æ³•

```
ç®—æ³•ï¼šInexact Newton-CG with Line Search

è¾“å…¥ï¼šåˆå§‹çŒœæµ‹ m_0, å®¹å·® tol, æœ€å¤§è¿­ä»£æ¬¡æ•° max_iter
è¾“å‡ºï¼šæœ€ä¼˜å‚æ•° m*

for k = 0, 1, 2, ... until convergence:
    1. æ±‚è§£æ­£é—®é¢˜ï¼š
       ç»™å®š m_kï¼Œæ±‚è§£ u_k
    
    2. æ±‚è§£ä¼´éšé—®é¢˜ï¼š
       ç»™å®š u_k, m_kï¼Œæ±‚è§£ p_k
    
    3. è®¡ç®—æ¢¯åº¦ï¼š
       g_k = âˆ‡_m J(m_k) = Î³R*m_k + C^T*p_k
       
       if ||g_k|| < tol:
           æ”¶æ•›ï¼Œé€€å‡º
    
    4. æ±‚è§£ Newton ç³»ç»Ÿï¼ˆç”¨ CGï¼‰ï¼š
       H_k * Î”m_k = -g_k
       
       å…¶ä¸­ H_k æ˜¯ Hessianï¼ˆæˆ– Gauss-Newton è¿‘ä¼¼ï¼‰
       
       CG å®¹å·®ï¼štol_cg = min(0.5, sqrt(||g_k||/||g_0||))
       ï¼ˆEisenstat-Walker å‡†åˆ™ï¼‰
    
    5. çº¿æœç´¢ï¼ˆArmijo è§„åˆ™ï¼‰ï¼š
       Î± = 1
       while J(m_k + Î±*Î”m_k) > J(m_k) + c*Î±*(g_k, Î”m_k):
           Î± = Î±/2
       
       m_{k+1} = m_k + Î±*Î”m_k
    
    6. æ£€æŸ¥æ”¶æ•›ï¼š
       if ||g_{k+1}|| < tol or |(g_k, Î”m_k)| < tol_gdm:
           æ”¶æ•›ï¼Œé€€å‡º
```

**å…³é”®æŠ€å·§**ï¼š

1. **Gauss-Newton è½¬ Newton**ï¼š
   - å‰å‡ æ¬¡è¿­ä»£ï¼ˆå¦‚å‰5æ¬¡ï¼‰ï¼šä½¿ç”¨ GN è¿‘ä¼¼ï¼ˆæ›´å¿«ï¼Œæ›´ç¨³å®šï¼‰
   - åç»­è¿­ä»£ï¼šä½¿ç”¨å®Œæ•´ Newtonï¼ˆäºŒæ¬¡æ”¶æ•›ï¼‰

2. **Eisenstat-Walker CG å®¹å·®**ï¼š
   ```python
   tolcg = min(0.5, sqrt(gradnorm / gradnorm_ini))
   ```
   è‡ªé€‚åº”è°ƒæ•´ CG ç²¾åº¦ï¼Œé¿å…è¿‡åº¦æ±‚è§£

3. **Steihaug è§„åˆ™**ï¼š
   CG é‡åˆ°è´Ÿæ›²ç‡æ—¶ï¼Œåœåœ¨ä¿¡èµ–åŸŸè¾¹ç•Œ

### 5.2 ä¼´éšæ–¹æ³•è®¡ç®—æ¢¯åº¦

**ç®—æ³•æ­¥éª¤**ï¼š

```python
def compute_gradient(m):
    """
    è¾“å…¥ï¼šå‚æ•° m
    è¾“å‡ºï¼šæ¢¯åº¦ âˆ‡J(m)
    """
    # 1. æ­£é—®é¢˜ï¼šæ±‚è§£ u(m)
    solve_forward(u, m)  # F(u, m, p) = 0 for all p
    
    # 2. ä¼´éšé—®é¢˜ï¼šæ±‚è§£ p(u, m)
    #    å³ç«¯é¡¹ï¼šâˆ‚J/âˆ‚u = W * (u - u_d)
    rhs = W * (u - u_d)
    solve_adjoint(p, u, m, rhs)  # âˆ‚F/âˆ‚u^T * p = -rhs
    
    # 3. ç»„è£…æ¢¯åº¦
    grad = Î³ * R * m + C^T * p
    #    = æ­£åˆ™åŒ–æ¢¯åº¦ + misfit æ¢¯åº¦
    
    return grad
```

**ä¸ºä»€ä¹ˆè¿™ä¹ˆåšï¼Ÿ**

ç›´æ¥æœ‰é™å·®åˆ†è®¡ç®—æ¢¯åº¦ï¼š
$$\frac{\partial J}{\partial m_i} \approx \frac{J(m + h e_i) - J(m)}{h}$$
éœ€è¦ $n$ æ¬¡æ­£é—®é¢˜æ±‚è§£ï¼ˆ$n$ = å‚æ•°ç»´åº¦ï¼‰

ä¼´éšæ–¹æ³•ï¼š
- 1 æ¬¡æ­£é—®é¢˜ + 1 æ¬¡ä¼´éšé—®é¢˜ = **ç»´åº¦ç‹¬ç«‹**ï¼

### 5.3 ä½ç§©åéªŒåæ–¹å·®è¿‘ä¼¼

åéªŒåæ–¹å·®ï¼š
$$\Gamma_{\text{post}} = (H_{\text{misfit}} + \Gamma_{\text{prior}}^{-1})^{-1}$$

**é—®é¢˜**ï¼š$H$ æ˜¯ $n \times n$ å¯†é›†çŸ©é˜µï¼ˆ$n \sim 10^4$-$10^6$ï¼‰ï¼Œæ— æ³•æ˜¾å¼å­˜å‚¨å’Œæ±‚é€†ï¼

**è§£å†³æ–¹æ¡ˆ**ï¼šä½ç§©è¿‘ä¼¼

1. **å¹¿ä¹‰ç‰¹å¾å€¼é—®é¢˜**ï¼š
   $$H_{\text{misfit}} V = \Gamma_{\text{prior}}^{-1} V \Lambda$$

2. **åªä¿ç•™å‰ $r$ ä¸ªå¤§ç‰¹å¾å€¼**ï¼š
   $$H_{\text{misfit}} \approx \Gamma_{\text{prior}}^{-1} V_r \Lambda_r V_r^T \Gamma_{\text{prior}}^{-1}$$

3. **Sherman-Morrison-Woodbury å…¬å¼**ï¼š
   $$\Gamma_{\text{post}} \approx \Gamma_{\text{prior}} - V_r D_r V_r^T$$
   
   å…¶ä¸­ $D_r = \text{diag}(\lambda_i/(\lambda_i+1))$

**å®ç°**ï¼š

```python
from hippylib import ReducedHessian, doublePassG

# 1. æ„å»º Hessian ç®—å­
Hmisfit = ReducedHessian(model, misfit_only=True)

# 2. éšæœºç‰¹å¾å€¼æ±‚è§£
r = 50  # ä¿ç•™ç‰¹å¾å€¼æ•°é‡
Omega = MultiVector(model.generate_vector(PARAMETER), r+10)
parRandom.normal(1., Omega)

d, V = doublePassG(
    Hmisfit,                    # Hessian ç®—å­
    prior.R,                    # å…ˆéªŒç²¾åº¦çŸ©é˜µ
    prior.Rsolver,              # å…ˆéªŒåæ–¹å·®çŸ©é˜µ
    Omega,                      # éšæœºå‘é‡
    r
)

# d: ç‰¹å¾å€¼ï¼ˆä»å¤§åˆ°å°ï¼‰
# V: ç‰¹å¾å‘é‡

# 3. ä»åéªŒé‡‡æ ·
prior_sample = dl.Vector()
prior.init_vector(prior_sample, 0)
prior.sample(noise, prior_sample)

# ä½ç§©æ ¡æ­£
post_sample = prior_sample.copy()
for i in range(r):
    correction = d[i]/(d[i]+1)
    Vip = V[i].inner(prior.R * prior_sample)
    post_sample.axpy(-correction * Vip, V[i])
```

---

## 6. å®è·µåº”ç”¨æŒ‡å—

### 6.1 å‚æ•°é€‰æ‹©æŒ‡å—

#### å…ˆéªŒå‚æ•°
```python
# LaplacianPrior: C = (Î´I - Î³Î”)^{-1}
gamma = 0.1   # â†‘ å¢åŠ  â†’ ç›¸å…³é•¿åº¦ â†‘ï¼ˆæ›´å…‰æ»‘ï¼‰
delta = 0.5   # â†‘ å¢åŠ  â†’ æ–¹å·® â†“ï¼ˆæ›´ç¡®å®šï¼‰

# ç›¸å…³é•¿åº¦å°ºåº¦çº¦ä¸º sqrt(Î³/Î´)
correlation_length = np.sqrt(gamma / delta)
```

**é€‰æ‹©å»ºè®®**ï¼š
- åœ°ä¸‹æ°´æµï¼š`gamma=0.1, delta=1.0`ï¼ˆä¸­ç­‰å…‰æ»‘ï¼‰
- çƒ­ä¼ å¯¼ï¼š`gamma=0.01, delta=0.1`ï¼ˆå¾ˆå…‰æ»‘ï¼‰
- åœ°éœ‡å­¦ï¼š`gamma=1.0, delta=10.0`ï¼ˆè¾ƒç²—ç³™ï¼‰

#### ä¼˜åŒ–å‚æ•°
```python
parameters["rel_tolerance"] = 1e-6    # ç›¸å¯¹æ¢¯åº¦å®¹å·®
parameters["abs_tolerance"] = 1e-9    # ç»å¯¹æ¢¯åº¦å®¹å·®
parameters["max_iter"] = 20           # æœ€å¤§è¿­ä»£æ¬¡æ•°
parameters["GN_iter"] = 5             # Gauss-Newton è¿­ä»£æ¬¡æ•°
parameters["cg_coarse_tolerance"] = 0.5  # CG æœ€ç²—å®¹å·®
parameters["cg_max_iter"] = 100       # CG æœ€å¤§è¿­ä»£æ¬¡æ•°
```

**è°ƒå‚æŠ€å·§**ï¼š
1. å¦‚æœä¸æ”¶æ•›ï¼Œå¢åŠ  `max_iter`
2. å¦‚æœ CG è¿­ä»£è¿‡å¤šï¼Œé™ä½ `cg_coarse_tolerance`
3. å¦‚æœæ—©æœŸéœ‡è¡ï¼Œå¢åŠ  `GN_iter`
4. å™ªå£°å¤§æ—¶ï¼Œç”¨ `GN_iter = max_iter`ï¼ˆå…¨ç”¨ GNï¼‰

### 6.2 å¸¸è§é—®é¢˜ä¸è§£å†³

#### é—®é¢˜ 1ï¼šæ­£é—®é¢˜æ±‚è§£å¤±è´¥
```
RuntimeError: Newton solver did not converge
```

**åŸå› **ï¼šå‚æ•°æ›´æ–°æ­¥é•¿å¤ªå¤§ï¼Œå¯¼è‡´æ­£é—®é¢˜éçº¿æ€§æ±‚è§£å¤±è´¥

**è§£å†³**ï¼š
```python
# æ–¹æ³• 1ï¼šå‡å°çº¿æœç´¢æ­¥é•¿
parameters["LS"]["c_armijo"] = 1e-3  # é»˜è®¤ 1e-4

# æ–¹æ³• 2ï¼šä½¿ç”¨ä¿¡èµ–åŸŸ
parameters["globalization"] = "TR"

# æ–¹æ³• 3ï¼šä½¿ç”¨æ›´ä¿å®ˆçš„åˆå§‹çŒœæµ‹
m0 = prior.mean  # ç”¨å…ˆéªŒå‡å€¼åˆå§‹åŒ–
```

#### é—®é¢˜ 2ï¼šæ¢¯åº¦ä¸ä¸‹é™
```
Iteration 5: ||grad|| = 1.234e-3 (not decreasing)
```

**åŸå› **ï¼š
- Hessian ä¸æ­£å®š
- CG æ±‚è§£å™¨ç²¾åº¦ä¸å¤Ÿ
- å‚æ•°ç©ºé—´ç—…æ€

**è§£å†³**ï¼š
```python
# å¢å¼ºæ­£å®šæ€§
parameters["GN_iter"] = max_iter  # å§‹ç»ˆç”¨ GN

# æé«˜ CG ç²¾åº¦
parameters["cg_coarse_tolerance"] = 0.1

# å¢åŠ æ­£åˆ™åŒ–
gamma *= 10  # å¢å¼ºå…ˆéªŒå¼ºåº¦
```

#### é—®é¢˜ 3ï¼šè®¡ç®—å¤ªæ…¢
```
æ¯æ¬¡è¿­ä»£éœ€è¦ 10 åˆ†é’Ÿ...
```

**åŠ é€ŸæŠ€å·§**ï¼š
```python
# 1. ä½¿ç”¨æ›´ç²—çš„ç½‘æ ¼
mesh = dl.UnitSquareMesh(32, 32)  # è€Œé 128x128

# 2. é™ä½ CG è¿­ä»£æ¬¡æ•°
parameters["cg_max_iter"] = 50

# 3. ä½¿ç”¨ LU åˆ†è§£ï¼ˆå°è§„æ¨¡é—®é¢˜ï¼‰
prior = LaplacianPrior(Vh, gamma, delta, solver_type="lu")

# 4. å¹¶è¡Œè¿è¡Œï¼ˆéœ€è¦ MPIï¼‰
mpirun -n 4 python your_script.py
```

### 6.3 éªŒè¯ä¸è°ƒè¯•

#### æ¢¯åº¦æ£€éªŒ
```python
from hippylib import modelVerify

# æœ‰é™å·®åˆ†éªŒè¯æ¢¯åº¦
h = 1e-6
err = modelVerify(model, x, h, 1)  # 1 = PARAMETER
print("Gradient error:", err)
# åº”è¯¥ < 1e-4
```

#### Hessian æ£€éªŒ
```python
# æœ‰é™å·®åˆ†éªŒè¯ Hessian
err_H = modelVerify(model, x, h, 2)  # 2 = Hessian
print("Hessian error:", err_H)
# åº”è¯¥ < 1e-3
```

#### æ”¶æ•›æ›²çº¿
```python
# è®°å½•æ¯æ¬¡è¿­ä»£
costs = []
grads = []

def callback(it, x):
    cost = model.cost(x)[0]
    grad = model.generate_vector(PARAMETER)
    gradnorm = model.evalGradientParameter(x, grad)
    costs.append(cost)
    grads.append(gradnorm)
    print(f"Iteration {it}: cost={cost:.3e}, ||grad||={gradnorm:.3e}")

solver = ReducedSpaceNewtonCG(model, parameters, callback=callback)
x = solver.solve(x)

# ç»˜åˆ¶
plt.figure(figsize=(12, 4))
plt.subplot(121)
plt.semilogy(costs)
plt.xlabel("Iteration")
plt.ylabel("Cost")
plt.subplot(122)
plt.semilogy(grads)
plt.xlabel("Iteration")
plt.ylabel("||Gradient||")
plt.show()
```

### 6.4 æ‰©å±•åˆ°è‡ªå·±çš„é—®é¢˜

#### æ¨¡æ¿ï¼šè‡ªå®šä¹‰ PDE é—®é¢˜

```python
# 1. å®šä¹‰ä½ çš„ PDE
def my_pde_varf(u, m, p):
    """
    å®šä¹‰ä½ çš„ PDE å¼±å½¢å¼
    
    ä¾‹å¦‚ï¼šéçº¿æ€§æ‰©æ•£
    -âˆ‡Â·(m^2 âˆ‡u) = f
    """
    return m**2 * ufl.inner(ufl.grad(u), ufl.grad(p)) * ufl.dx - f * p * ufl.dx

# 2. è®¾ç½®è¾¹ç•Œæ¡ä»¶
def boundary_func(x, on_boundary):
    return on_boundary and x[0] < dl.DOLFIN_EPS  # å·¦è¾¹ç•Œ

bc = dl.DirichletBC(Vh[STATE], u_boundary_value, boundary_func)

# 3. åˆ›å»º PDE é—®é¢˜
pde = PDEVariationalProblem(
    Vh, my_pde_varf, bc, bc0,
    is_fwd_linear=False  # éçº¿æ€§ï¼
)

# 4. å…¶ä½™æ­¥éª¤ç›¸åŒ
prior = BiLaplacianPrior(Vh[PARAMETER], gamma, delta)
misfit = DiscreteStateObservation(B, data, noise_variance)
model = Model(pde, prior, misfit)
# ... æ±‚è§£
```

#### æ¨¡æ¿ï¼šè‡ªå®šä¹‰è§‚æµ‹ç®—å­

```python
class MyCustomObservation(Misfit):
    """è‡ªå®šä¹‰è§‚æµ‹"""
    def __init__(self, targets, data, noise_var):
        self.targets = targets  # è§‚æµ‹ä½ç½®
        self.data = data        # è§‚æµ‹æ•°æ®
        self.noise_var = noise_var
        
        # æ„å»ºè§‚æµ‹çŸ©é˜µ B
        self.B = self._build_observation_operator()
        self.Bu = dl.Vector()
        self.B.init_vector(self.Bu, 0)
    
    def _build_observation_operator(self):
        # å®ç°ä½ çš„è§‚æµ‹ç®—å­
        # ä¾‹å¦‚ï¼šç§¯åˆ†è§‚æµ‹ã€è¾¹ç•Œé€šé‡ç­‰
        pass
    
    def cost(self, x):
        self.B.mult(x[STATE], self.Bu)
        diff = self.Bu - self.data
        return 0.5 / self.noise_var * diff.inner(diff)
    
    def grad(self, i, x, out):
        if i == STATE:
            self.B.mult(x[STATE], self.Bu)
            diff = self.Bu - self.data
            self.B.transpmult(diff, out)
            out *= 1.0 / self.noise_var
        else:
            out.zero()
    
    def setLinearizationPoint(self, x, gauss_newton_approx=False):
        pass
    
    def apply_ij(self, i, j, dir, out):
        if i == STATE and j == STATE:
            self.B.mult(dir, self.Bu)
            self.B.transpmult(self.Bu, out)
            out *= 1.0 / self.noise_var
        else:
            out.zero()
```

---

## 7. é«˜çº§ä¸»é¢˜

### 7.1 è´å¶æ–¯åæ¼”

ä»ç¡®å®šæ€§åæ¼”åˆ°è´å¶æ–¯ï¼š

**ç¡®å®šæ€§**ï¼šå¯»æ‰¾å•ä¸€æœ€ä¼˜å‚æ•°
$$\hat{m} = \arg\min J(m)$$

**è´å¶æ–¯**ï¼šè®¡ç®—å‚æ•°çš„åéªŒåˆ†å¸ƒ
$$\pi(m | d) \propto \pi(d | m) \pi(m)$$

**Laplace è¿‘ä¼¼**ï¼š
$$\pi(m | d) \approx \mathcal{N}(m_{MAP}, \Gamma_{post})$$

å…¶ä¸­ï¼š
- $m_{MAP}$ï¼šæœ€å¤§åéªŒä¼°è®¡ï¼ˆåŒç¡®å®šæ€§è§£ï¼‰
- $\Gamma_{post}$ï¼šåéªŒåæ–¹å·®

**å®ç°**ï¼š
```python
# 1. æ±‚ MAP ç‚¹
solver = ReducedSpaceNewtonCG(model)
x = solver.solve(x)
m_MAP = x[PARAMETER]

# 2. æ„å»ºåéªŒåæ–¹å·®
from hippylib import Posterior, LowRankHessian

posterior = Posterior(model)
posterior.setLinearizationPoint(x)

# 3. è®¡ç®—ä½ç§© Hessian
Hmisfit = LowRankHessian(posterior, r=50)

# 4. åéªŒé‡‡æ ·
samples = [dl.Vector() for _ in range(100)]
for s in samples:
    posterior.init_vector(s, 0)
    posterior.sample(noise, s)
```

### 7.2 æ—¶é—´ä¾èµ–é—®é¢˜

```python
from hippylib import TimeDependentPDEVariationalProblem

# å®šä¹‰æ—¶é—´ä¾èµ–çš„ PDE
def pde_varf_tv(u, m, p):
    u_t, u_x = u  # æ—¶é—´å¯¼æ•°ï¼Œç©ºé—´è§£
    return (
        u_t * p * ufl.dx +
        ufl.exp(m) * ufl.inner(ufl.grad(u_x), ufl.grad(p)) * ufl.dx
    )

# è®¾ç½®æ—¶é—´ç¦»æ•£
T = 1.0
dt = 0.01
pde = TimeDependentPDEVariationalProblem(
    Vh, pde_varf_tv, bc, bc0,
    T=T, dt=dt, theta=0.5  # Crank-Nicolson
)
```

### 7.3 å¹¶è¡Œè®¡ç®—

hippylib è‡ªåŠ¨æ”¯æŒ MPI å¹¶è¡Œï¼š

```bash
# å•æœºå¤šæ ¸
mpirun -n 8 python inverse_problem.py

# é›†ç¾¤
srun -n 256 python inverse_problem.py
```

ä»£ç æ— éœ€ä¿®æ”¹ï¼FEniCS å’Œ PETSc ä¼šè‡ªåŠ¨å¤„ç†ï¼š
- ç½‘æ ¼åˆ†åŒº
- çŸ©é˜µ/å‘é‡åˆ†å¸ƒ
- å¹¶è¡Œçº¿æ€§æ±‚è§£å™¨

---

## 8. æ€»ç»“ä¸æœ€ä½³å®è·µ

### æ ¸å¿ƒè¦ç‚¹

1. **ä¼´éšæ–¹æ³•æ˜¯å…³é”®**
   - ç»´åº¦ç‹¬ç«‹çš„æ¢¯åº¦è®¡ç®—
   - ä¸€æ¬¡æ­£é—®é¢˜ + ä¸€æ¬¡ä¼´éšé—®é¢˜

2. **Hessian ä¸éœ€è¦æ˜¾å¼æ„é€ **
   - é€šè¿‡å¢é‡é—®é¢˜å®ç° Hessian-å‘é‡ä¹˜ç§¯
   - Gauss-Newton è¿‘ä¼¼åŠ é€ŸåˆæœŸè¿­ä»£

3. **ä½ç§©è¿‘ä¼¼é™ç»´**
   - åéªŒåæ–¹å·®çš„ä½ç§©è¡¨ç¤º
   - åªéœ€è¦å‡ åä¸ªç‰¹å¾å‘é‡

4. **å…ˆéªŒå¾ˆé‡è¦**
   - æ§åˆ¶è§£çš„å…‰æ»‘æ€§
   - å¹³è¡¡æ•°æ®æ‹Ÿåˆå’Œæ­£åˆ™åŒ–

### å·¥ä½œæµç¨‹æ£€æŸ¥æ¸…å•

- [ ] ç½‘æ ¼å’Œå‡½æ•°ç©ºé—´è®¾ç½®æ­£ç¡®
- [ ] PDE å¼±å½¢å¼éªŒè¯ï¼ˆæ‰‹åŠ¨æ±‚è§£ä¸€æ¬¡ï¼‰
- [ ] è¾¹ç•Œæ¡ä»¶æ­£ç¡®æ–½åŠ 
- [ ] è§‚æµ‹æ•°æ®è´¨é‡æ£€æŸ¥ï¼ˆå™ªå£°æ°´å¹³ï¼‰
- [ ] å…ˆéªŒå‚æ•°åˆç†ï¼ˆcorrelation lengthï¼‰
- [ ] åˆå§‹çŒœæµ‹ä¸ä¼šå¯¼è‡´æ­£é—®é¢˜å¤±è´¥
- [ ] æ¢¯åº¦éªŒè¯é€šè¿‡ï¼ˆæœ‰é™å·®åˆ†æ£€æŸ¥ï¼‰
- [ ] Hessian éªŒè¯é€šè¿‡ï¼ˆå¯¹å°é—®é¢˜ï¼‰
- [ ] ä¼˜åŒ–å‚æ•°è°ƒä¼˜ï¼ˆGN_iter, toleranceï¼‰
- [ ] æ”¶æ•›æ›²çº¿åˆç†ï¼ˆå•è°ƒä¸‹é™ï¼‰
- [ ] ç»“æœå¯è§†åŒ–å’Œç‰©ç†æ£€éªŒ

### å¸¸ç”¨ä»£ç ç‰‡æ®µ

```python
# å¿«é€Ÿè®¾ç½®æ ‡å‡†åé—®é¢˜
def setup_standard_inverse_problem(mesh, gamma, delta, obs_points, data, noise_var):
    # å‡½æ•°ç©ºé—´
    Vh_state = dl.FunctionSpace(mesh, 'Lagrange', 2)
    Vh_param = dl.FunctionSpace(mesh, 'Lagrange', 1)
    Vh = [Vh_state, Vh_param, Vh_state]
    
    # PDE
    bc = dl.DirichletBC(Vh[STATE], dl.Constant(0.0), "on_boundary")
    def pde_varf(u, m, p):
        return ufl.exp(m) * ufl.inner(ufl.grad(u), ufl.grad(p)) * ufl.dx
    pde = PDEVariationalProblem(Vh, pde_varf, bc, bc, is_fwd_linear=True)
    
    # å…ˆéªŒ
    prior = BiLaplacianPrior(Vh[PARAMETER], gamma, delta)
    
    # Misfit
    B = assemblePointwiseObservation(Vh[STATE], obs_points)
    misfit = DiscreteStateObservation(B, data, noise_var)
    
    # Model
    model = Model(pde, prior, misfit)
    
    return model, Vh

# ä½¿ç”¨
model, Vh = setup_standard_inverse_problem(
    mesh, gamma=0.1, delta=0.5,
    obs_points=targets, data=observations,
    noise_var=0.01
)
```

---

## å‚è€ƒèµ„æº

1. **hippylib æ–‡æ¡£**: https://hippylib.readthedocs.io
2. **æ•™ç¨‹**: `../hippylib/tutorial/`
   - `2_PoissonDeterministic.ipynb`: ç¡®å®šæ€§åæ¼”
   - `3_SubsurfaceBayesian.ipynb`: è´å¶æ–¯åæ¼”
   - `4_AdvectionDiffusionBayesian.ipynb`: æ—¶é—´ä¾èµ–
3. **è®ºæ–‡**:
   - Villa et al., "hIPPYlib: An Extensible Software Framework for Large-Scale Inverse Problems", JOSS 2018
4. **FEniCS æ–‡æ¡£**: https://fenicsproject.org
5. **ä½ çš„ä»£ç **: `hippytest.py` - æ‰‹åŠ¨å®ç°çš„ä¼˜ç§€å‚è€ƒï¼

---

## ä¸‹ä¸€æ­¥è®¡åˆ’

å°† hippylib åº”ç”¨åˆ°ä½ çš„ PiX é¡¹ç›®ï¼š

1. **å‚æ•°è¯†åˆ«**ï¼šä» PDE æ•°æ®åæ¨ç‰©ç†å‚æ•°
2. **æ–¹ç¨‹å‘ç°**ï¼šç»“åˆç¬¦å·å›å½’ï¼Œè¯†åˆ« PDE å½¢å¼
3. **ä¸ç¡®å®šæ€§é‡åŒ–**ï¼šé‡åŒ–è¯†åˆ«å‚æ•°çš„ä¸ç¡®å®šæ€§
4. **æ•°æ®åŒåŒ–**ï¼šèåˆè§‚æµ‹æ•°æ®æ”¹è¿› PDE æ¨¡å‹

è¿™äº›éƒ½å¯ä»¥åˆ©ç”¨ hippylib çš„å¼ºå¤§åŠŸèƒ½ï¼
